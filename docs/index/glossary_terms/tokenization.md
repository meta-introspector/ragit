# Tokenization

The process of breaking down text into smaller units (tokens), used in Ragit's chunking process.